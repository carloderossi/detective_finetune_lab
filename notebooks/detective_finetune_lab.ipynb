{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195715,
     "status": "ok",
     "timestamp": 1771191226822,
     "user": {
      "displayName": "Carlo Carlo",
      "userId": "11471837763026793486"
     },
     "user_tz": -60
    },
    "id": "EvPeuYNuaD3-",
    "outputId": "e338b91a-ea9a-473c-98b7-b5013963fe46"
   },
   "outputs": [],
   "source": [
    "# --- CLEAN START ---\n",
    "# Remove everything that can conflict\n",
    "# !pip uninstall -y unsloth unsloth-zoo transformers accelerate peft datasets tokenizers huggingface-hub bitsandbytes sentencepiece\n",
    "\n",
    "# --- PYTORCH (CUDA 11.8) ---\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# --- INSTALL MODERN UNSLOTH (the correct one for Qwen2.5) ---\n",
    "# !pip install \"unsloth==2026.2.1\"\n",
    "\n",
    "# --- INSTALL THE HF STACK COMPATIBLE WITH UNSLOTH 2026.x ---\n",
    "# !pip install \"transformers>=4.57.0\" \"accelerate>=0.34.1\" \"peft>=0.18.0\" datasets sentencepiece bitsandbytes\n",
    "\n",
    "# 1. Install Unsloth with the latest Colab-specific optimization\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# 2. Install essential backends for LoRA and 4-bit quantization\n",
    "!pip install --no-deps \"xformers<0.0.30\" \"trl<0.13.0\" peft accelerate bitsandbytes\n",
    "\n",
    "# 3. Install data handling and high-speed download utilities\n",
    "!pip install datasets sentencepiece hf_transfer huggingface_hub\n",
    "\n",
    "# 4. Enable HF_TRANSFER for 10x faster model loading into Colab's local disk\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2153,
     "status": "ok",
     "timestamp": 1771191228972,
     "user": {
      "displayName": "Carlo Carlo",
      "userId": "11471837763026793486"
     },
     "user_tz": -60
    },
    "id": "XxS1ftFIdXRp",
    "outputId": "fbff2aa2-43bb-40f0-919a-50083fbf33ee"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import torch\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "with open('/content/drive/MyDrive/TMP/HF_TOKEN.txt') as f:\n",
    "    token = f.read().strip()\n",
    "    if token:\n",
    "        os.environ[\"HF_TOKEN\"] = token\n",
    "# 2. Check if HF_TOKEN is set in the environment\n",
    "test_token=os.getenv(\"HF_TOKEN\")\n",
    "if test_token:\n",
    "    print(\"HF_TOKEN is set in the environment.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN is NOT set in the environment. Please check your token file and try again.\")\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables.\")\n",
    "\n",
    "# 2. Detect Hardware for optimization\n",
    "major_v, _ = torch.cuda.get_device_capability()\n",
    "IS_A100 = True if major_v >= 8 else False\n",
    "print(f\"üöÄ Hardware: {'A100 (using BF16)' if IS_A100 else 'T4 (using FP16)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "paths = {\n",
    "    \"Script\": \"/content/drive/MyDrive/scripts/train_scout_unsloth.py\",\n",
    "    \"Data\": \"/content/drive/MyDrive/data/train/detective_finetune.jsonl\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {name} found: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} MISSING: {path}\")\n",
    "        raise FileNotFoundError(f\"{name} not found at {path}. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8903,
     "status": "ok",
     "timestamp": 1771191237876,
     "user": {
      "displayName": "Carlo Carlo",
      "userId": "11471837763026793486"
     },
     "user_tz": -60
    },
    "id": "4sgKZN4RdpHr",
    "outputId": "2ef113e8-730b-4037-91a7-32d53f601428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-16 18:36:52.814618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771267013.006387    1263 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771267013.069093    1263 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771267013.515901    1263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771267013.515939    1263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771267013.515943    1263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771267013.515947    1263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-16 18:36:53.555840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model Qwen/Qwen2.5-7B-Instruct with Unsloth...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "model.safetensors.index.json: 112kB [00:00, 194MB/s]\n",
      "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:37<00:00, 134MB/s]     \n",
      "model-00002-of-00002.safetensors: 100% 2.16G/2.16G [00:24<00:00, 86.7MB/s]   \n",
      "Loading checkpoint shards: 100% 2/2 [00:25<00:00, 12.88s/it]\n",
      "generation_config.json: 100% 271/271 [00:00<00:00, 1.67MB/s]\n",
      "tokenizer_config.json: 7.36kB [00:00, 30.0MB/s]\n",
      "vocab.json: 2.78MB [00:00, 72.2MB/s]\n",
      "merges.txt: 1.67MB [00:00, 144MB/s]\n",
      "added_tokens.json: 100% 605/605 [00:00<00:00, 5.52MB/s]\n",
      "special_tokens_map.json: 100% 614/614 [00:00<00:00, 5.77MB/s]\n",
      "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 56.7MB/s]\n",
      "Pad token set to: <|im_end|> (id=151645)\n",
      "Creating PEFT model using Unsloth‚Äôs built‚Äëin LoRA config helper...\n",
      "Unsloth 2026.2.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "PEFT model created with LoRA adapters.\n",
      "Loading dataset from data/train/detective_finetune.jsonl...\n",
      "Generating train split: 205 examples [00:00, 1924.58 examples/s]\n",
      "Total examples: 205\n",
      "Map: 100% 205/205 [00:00<00:00, 5368.18 examples/s]\n",
      "Tokenizing...\n",
      "Map: 100% 194/194 [00:03<00:00, 54.53 examples/s]\n",
      "Map: 100% 11/11 [00:00<00:00, 69.17 examples/s]\n",
      "Packing sequences...\n",
      "Map: 100% 194/194 [00:01<00:00, 131.81 examples/s]\n",
      "Map: 100% 11/11 [00:00<00:00, 323.79 examples/s]\n",
      "Packed train samples: 274\n",
      "Packed eval samples:  11\n",
      "Starting training...\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 274 | Num Epochs = 3 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 7,696,356,864 (1.05% trained)\n",
      "  0% 0/100 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 2.1374, 'grad_norm': 6.395338914444437e-06, 'learning_rate': 0.00011999999999999999, 'epoch': 0.15}\n",
      "{'loss': 2.1952, 'grad_norm': 6.611326625716174e-06, 'learning_rate': 0.00014934480519952312, 'epoch': 0.29}\n",
      "{'loss': 2.1742, 'grad_norm': 6.9122042987146415e-06, 'learning_rate': 0.0001467026836150768, 'epoch': 0.44}\n",
      "{'loss': 2.1889, 'grad_norm': 8.162634912878275e-06, 'learning_rate': 0.00014210469973655724, 'epoch': 0.58}\n",
      "{'loss': 2.1614, 'grad_norm': 1.0537649359321222e-05, 'learning_rate': 0.00013567627457812106, 'epoch': 0.73}\n",
      "{'loss': 2.1568, 'grad_norm': 1.1040823665098287e-05, 'learning_rate': 0.00012759275882043663, 'epoch': 0.88}\n",
      "{'loss': 2.1314, 'grad_norm': 9.40315021580318e-06, 'learning_rate': 0.00011807464970122076, 'epoch': 1.0}\n",
      " 35% 35/100 [53:48<1:18:59, 72.91s/it]Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "\n",
      "  0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
      " 18% 2/11 [00:01<00:08,  1.08it/s]\u001b[A\n",
      " 27% 3/11 [00:05<00:16,  2.07s/it]\u001b[A\n",
      " 36% 4/11 [00:09<00:18,  2.66s/it]\u001b[A\n",
      " 45% 5/11 [00:12<00:18,  3.01s/it]\u001b[A\n",
      " 55% 6/11 [00:16<00:16,  3.23s/it]\u001b[A\n",
      " 64% 7/11 [00:20<00:13,  3.38s/it]\u001b[A\n",
      " 73% 8/11 [00:23<00:10,  3.46s/it]\u001b[A\n",
      " 82% 9/11 [00:27<00:07,  3.53s/it]\u001b[A\n",
      " 91% 10/11 [00:31<00:03,  3.57s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.1717584133148193, 'eval_runtime': 40.6711, 'eval_samples_per_second': 0.27, 'eval_steps_per_second': 0.27, 'epoch': 1.0}\n",
      " 35% 35/100 [54:29<1:18:59, 72.91s/it]\n",
      "100% 11/11 [00:36<00:00,  3.60s/it]\u001b[A\n",
      "{'loss': 2.2108, 'grad_norm': 1.2742588296532631e-05, 'learning_rate': 0.00010738157642538695, 'epoch': 1.15}\n",
      "{'loss': 2.1543, 'grad_norm': 1.1272920346527826e-05, 'learning_rate': 9.580521815713833e-05, 'epoch': 1.29}\n",
      "{'loss': 2.1984, 'grad_norm': 9.290834896091837e-06, 'learning_rate': 8.366134777271897e-05, 'epoch': 1.44}\n",
      "{'loss': 2.1611, 'grad_norm': 9.159291039395612e-06, 'learning_rate': 7.128121839950831e-05, 'epoch': 1.58}\n",
      "{'loss': 2.2136, 'grad_norm': 1.0984314940287732e-05, 'learning_rate': 5.900252769421899e-05, 'epoch': 1.73}\n",
      "{'loss': 2.2252, 'grad_norm': 9.689525541034527e-06, 'learning_rate': 4.716020633113343e-05, 'epoch': 1.88}\n",
      "{'loss': 2.1968, 'grad_norm': 9.448890523344744e-06, 'learning_rate': 3.607728196640888e-05, 'epoch': 2.0}\n",
      " 70% 70/100 [1:48:24<36:27, 72.93s/it]\n",
      "  0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
      " 18% 2/11 [00:03<00:16,  1.84s/it]\u001b[A\n",
      " 27% 3/11 [00:07<00:20,  2.60s/it]\u001b[A\n",
      " 36% 4/11 [00:11<00:20,  3.00s/it]\u001b[A\n",
      " 45% 5/11 [00:14<00:19,  3.23s/it]\u001b[A\n",
      " 55% 6/11 [00:18<00:16,  3.37s/it]\u001b[A\n",
      " 64% 7/11 [00:22<00:13,  3.47s/it]\u001b[A\n",
      " 73% 8/11 [00:25<00:10,  3.53s/it]\u001b[A\n",
      " 82% 9/11 [00:29<00:07,  3.58s/it]\u001b[A\n",
      " 91% 10/11 [00:33<00:03,  3.60s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.190462827682495, 'eval_runtime': 40.3326, 'eval_samples_per_second': 0.273, 'eval_steps_per_second': 0.273, 'epoch': 2.0}\n",
      " 70% 70/100 [1:49:04<36:27, 72.93s/it]\n",
      "100% 11/11 [00:38<00:00,  3.62s/it]\u001b[A\n",
      "{'loss': 2.1789, 'grad_norm': 9.42966653383337e-06, 'learning_rate': 2.6056067885687346e-05, 'epoch': 2.15}\n",
      " 79% 79/100 [2:03:50<33:05, 94.52s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "script_path = \"/content/drive/MyDrive/scripts/train_scout_unsloth.py\"\n",
    "\n",
    "print(\"üé¨ Initializing fine-tuning session...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the training script\n",
    "!python {script_path}\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_mins = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÅ TRAINING SESSION COMPLETE\")\n",
    "print(f\"‚è±Ô∏è Total Time: {elapsed_mins:.2f} minutes\")\n",
    "print(f\"üìÇ Output Location: /content/drive/MyDrive/outputs/detective-qwen-sft\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNj/C8zFI15+TQidkQb9Lvv",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
