{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195715,
     "status": "ok",
     "timestamp": 1771191226822,
     "user": {
      "displayName": "Carlo Carlo",
      "userId": "11471837763026793486"
     },
     "user_tz": -60
    },
    "id": "EvPeuYNuaD3-",
    "outputId": "e338b91a-ea9a-473c-98b7-b5013963fe46"
   },
   "outputs": [],
   "source": [
    "# --- CLEAN START ---\n",
    "# Remove everything that can conflict\n",
    "# !pip uninstall -y unsloth unsloth-zoo transformers accelerate peft datasets tokenizers huggingface-hub bitsandbytes sentencepiece\n",
    "\n",
    "# --- PYTORCH (CUDA 11.8) ---\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# --- INSTALL MODERN UNSLOTH (the correct one for Qwen2.5) ---\n",
    "# !pip install \"unsloth==2026.2.1\"\n",
    "\n",
    "# --- INSTALL THE HF STACK COMPATIBLE WITH UNSLOTH 2026.x ---\n",
    "# !pip install \"transformers>=4.57.0\" \"accelerate>=0.34.1\" \"peft>=0.18.0\" datasets sentencepiece bitsandbytes\n",
    "\n",
    "# Clean uninstall - remove everything that could conflict\n",
    "!pip uninstall -y unsloth unsloth-zoo transformers accelerate peft datasets tokenizers huggingface-hub bitsandbytes sentencepiece trl xformers\n",
    "\n",
    "# Install pinned PyTorch for CUDA 11.8 (good for T4)\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install exact Unsloth 2026.2.1 release\n",
    "!pip install \"unsloth==2026.2.1\"\n",
    "\n",
    "# Force-reinstall unsloth_zoo right after to match the 2026.2.1 expectations\n",
    "!pip install --no-cache-dir --force-reinstall \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git@February-2026\"\n",
    "\n",
    "# Compatible HF ecosystem versions (your original bounds are fine, but explicit helps)\n",
    "!pip install \"transformers>=4.57.0,<4.58.0\" \"accelerate>=0.34.1\" \"peft>=0.18.0\" \"datasets\" \"sentencepiece\" \"bitsandbytes\" \"trl<0.25.0\"\n",
    "\n",
    "##########################################\n",
    "# 1. Install Unsloth with the latest Colab-specific optimization\n",
    "# !pip install --force-reinstall --no-cache-dir --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-cache-dir unsloth_zoo\n",
    "# Force-update both packages together to stay in sync\n",
    "# !pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --upgrade --no-cache-dir unsloth_zoo\n",
    "# !pip uninstall -y unsloth unsloth-zoo\n",
    "# !pip install --upgrade --no-cache-dir \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --upgrade --no-cache-dir unsloth_zoo\n",
    "\n",
    "# 2. Install essential backends for LoRA and 4-bit quantization\n",
    "#!pip install --no-deps \"xformers<0.0.30\" \"trl<0.13.0\" peft accelerate bitsandbytes\n",
    "\n",
    "# 3. Install data handling and high-speed download utilities\n",
    "#!pip install datasets sentencepiece hf_transfer huggingface_hub\n",
    "\n",
    "# 4. Enable HF_TRANSFER for 10x faster model loading into Colab's local disk\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth; print(unsloth.__version__)\n",
    "import unsloth_zoo; print(unsloth_zoo.__version__ if hasattr(unsloth_zoo, '__version__') else \"no __version__ attr\")\n",
    "!pip show unsloth unsloth-zoo | grep -E \"Name|Version|Location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2153,
     "status": "ok",
     "timestamp": 1771191228972,
     "user": {
      "displayName": "Carlo Carlo",
      "userId": "11471837763026793486"
     },
     "user_tz": -60
    },
    "id": "XxS1ftFIdXRp",
    "outputId": "fbff2aa2-43bb-40f0-919a-50083fbf33ee"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import torch\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "with open('/content/drive/MyDrive/TMP/HF_TOKEN.txt') as f:\n",
    "    token = f.read().strip()\n",
    "    if token:\n",
    "        os.environ[\"HF_TOKEN\"] = token\n",
    "# 2. Check if HF_TOKEN is set in the environment\n",
    "test_token=os.getenv(\"HF_TOKEN\")\n",
    "if test_token:\n",
    "    print(\"HF_TOKEN is set in the environment.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN is NOT set in the environment. Please check your token file and try again.\")\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables.\")\n",
    "\n",
    "# 2. Detect Hardware for optimization\n",
    "major_v, _ = torch.cuda.get_device_capability()\n",
    "IS_A100 = True if major_v >= 8 else False\n",
    "print(f\"üöÄ Hardware: {'A100 (using BF16)' if IS_A100 else 'T4 (using FP16)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "paths = {\n",
    "    \"Script\": \"/content/drive/MyDrive/scripts/train_scout_unsloth.py\",\n",
    "    \"Data\": \"/content/drive/MyDrive/data/train/detective_finetune.jsonl\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {name} found: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} MISSING: {path}\")\n",
    "        raise FileNotFoundError(f\"{name} not found at {path}. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8903,
     "status": "ok",
     "timestamp": 1771191237876,
     "user": {
      "displayName": "Carlo Carlo",
      "userId": "11471837763026793486"
     },
     "user_tz": -60
    },
    "id": "4sgKZN4RdpHr",
    "outputId": "2ef113e8-730b-4037-91a7-32d53f601428"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "# Force memory management fixes before running the script\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# Force torch to avoid using the broken dynamo compiler\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "script_path = \"/content/drive/MyDrive/scripts/train_scout_unsloth.py\"\n",
    "\n",
    "print(\"üé¨ Initializing fine-tuning session...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the training script\n",
    "!python {script_path}\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_mins = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÅ TRAINING SESSION COMPLETE\")\n",
    "print(f\"‚è±Ô∏è Total Time: {elapsed_mins:.2f} minutes\")\n",
    "print(f\"üìÇ Output Location: /content/drive/MyDrive/outputs/detective-qwen-sft\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNj/C8zFI15+TQidkQb9Lvv",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
